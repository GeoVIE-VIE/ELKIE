# Cowrie LLM Responder - High-Interaction SSH Honeypot
# Optimized for CPU-based LLM inference on Xeon Platinum 8168

FROM python:3.11-slim-bookworm

LABEL maintainer="ELKIE Honeypot Project"
LABEL description="SSH Honeypot with Local LLM Response Generation"

# Install system dependencies
RUN apt-get update && apt-get install -y --no-install-recommends \
    openssh-client \
    curl \
    && rm -rf /var/lib/apt/lists/*

# Create app directory
WORKDIR /app

# Install Python dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy application code
COPY llm_client.py .
COPY cowrie_llm_responder.py .
COPY prompts/ ./prompts/

# Create log directory
RUN mkdir -p /var/log/cowrie-llm

# Create non-root user for security (but we'll need root for port binding)
# The honeypot itself runs as a limited user after binding

# Environment defaults
ENV PYTHONUNBUFFERED=1
ENV LOG_LEVEL=INFO
ENV SSH_PORT=8022
ENV TELNET_PORT=8023
ENV LLM_MODEL=mistral:7b-instruct-v0.2-q4_K_M
ENV OLLAMA_HOST=http://ollama:11434

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \
    CMD curl -f http://localhost:${SSH_PORT} || exit 1

# Expose ports
EXPOSE 8022 8023

# Run the honeypot
CMD ["python", "cowrie_llm_responder.py"]
